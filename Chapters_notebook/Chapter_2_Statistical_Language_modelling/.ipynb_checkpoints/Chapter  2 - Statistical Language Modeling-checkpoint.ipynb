{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational linguistics is an emerging field that is widely used in analytics, software applications, and contexts where people communicate with machines. Computational linguistics may be defined as a subfield of artificial intelligence. Applications of computational linguistics include machine translation, speech recognition, intelligent Web searching, information retrieval, and intelligent spelling checkers. It is important to understand the preprocessing tasks or the computations that can be performed on natural language text. In the following chapter, we will discuss ways to calculate word frequencies, the Maximum Likelihood Estimation (MLE) model, interpolation on data, and so on. But first, let's go through the various topics that we will cover in this chapter. They are as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•Calculating word frequencies (1-gram, 2-gram, 3-gram)\n",
    "\n",
    "•Developing MLE for a given text\n",
    "\n",
    "•Applying smoothing on the MLE model\n",
    "\n",
    "•Developing a back-off mechanism for MLE\n",
    "\n",
    "•Applying interpolation on data to get a mix and match\n",
    "\n",
    "•Evaluating a language model through perplexity\n",
    "\n",
    "•Applying Metropolis-Hastings in modeling languages\n",
    "\n",
    "•Applying Gibbs sampling in language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations may be defined as the collection of two or more tokens that tend to exist together. For example, the United States, the United Kingdom, Union of Soviet Socialist Republics, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['De', 'verzekeringsmaatschappijen', 'verhelen', ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import alpino\n",
    "alpino.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'\", 's'),\n",
       " ('arthur', ':'),\n",
       " ('#', '1'),\n",
       " (\"'\", 't'),\n",
       " ('villager', '#'),\n",
       " ('#', '2'),\n",
       " (']', '['),\n",
       " ('1', ':'),\n",
       " ('oh', ','),\n",
       " ('black', 'knight')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.corpus import webtext\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "tokens=[t.lower() for t in webtext.words('grail.txt')]\n",
    "words=BigramCollocationFinder.from_words(tokens)\n",
    "words.nbest(BigramAssocMeasures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('black', 'knight'),\n",
       " ('clop', 'clop'),\n",
       " ('head', 'knight'),\n",
       " ('mumble', 'mumble'),\n",
       " ('squeak', 'squeak'),\n",
       " ('saw', 'saw'),\n",
       " ('holy', 'grail'),\n",
       " ('run', 'away'),\n",
       " ('french', 'guard'),\n",
       " ('cartoon', 'character'),\n",
       " ('iesu', 'domine'),\n",
       " ('pie', 'iesu'),\n",
       " ('round', 'table'),\n",
       " ('sir', 'robin'),\n",
       " ('clap', 'clap')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "stop = (stopwords.words('english'))\n",
    "stops_filter = lambda w: len(w) < 3 or w in stop\n",
    "tokens=[t.lower() for t in webtext.words('grail.txt')]\n",
    "words=BigramCollocationFinder.from_words(tokens)\n",
    "words.apply_word_filter(stops_filter)\n",
    "words.nbest(BigramAssocMeasures.likelihood_ratio, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 'Never'),\n",
       " ('Hardwork', 'is'),\n",
       " ('Never', 'give'),\n",
       " ('give', 'up'),\n",
       " ('is', 'the'),\n",
       " ('key', 'to'),\n",
       " ('success', '.'),\n",
       " ('the', 'key'),\n",
       " ('to', 'success'),\n",
       " ('up', '!')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1=\"Hardwork is the key to success. Never give up!\"\n",
    "word = nltk.wordpunct_tokenize(text1)\n",
    "finder = BigramCollocationFinder.from_words(word)\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "value = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "sorted(bigram for bigram, score in value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hello', 'how', 'are', 'you') 2\n",
      "('how', 'are', 'you', 'doing') 1\n",
      "('are', 'you', 'doing', '?') 1\n",
      "('you', 'doing', '?', 'I') 1\n",
      "('doing', '?', 'I', 'hope') 1\n",
      "('?', 'I', 'hope', 'you') 1\n",
      "('I', 'hope', 'you', 'find') 1\n",
      "('hope', 'you', 'find', 'the') 1\n",
      "('you', 'find', 'the', 'book') 1\n",
      "('find', 'the', 'book', 'interesting') 1\n",
      "('the', 'book', 'interesting', 'Hello') 1\n",
      "('book', 'interesting', 'Hello', 'how') 1\n",
      "('interesting', 'Hello', 'how', 'are') 1\n"
     ]
    }
   ],
   "source": [
    "text=\"Hello how are you doing ? I hope you find the book interesting Hello how are you\"\n",
    "tokens=nltk.wordpunct_tokenize(text)\n",
    "fourgrams=nltk.collocations.QuadgramCollocationFinder.from_words(tokens)\n",
    "for fourgram, freq in fourgrams.ngram_fd.items():\n",
    "    print(fourgram,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hello', ',', 'please', 'read', 'the')\n",
      "(',', 'please', 'read', 'the', 'book')\n",
      "('please', 'read', 'the', 'book', 'thoroughly')\n",
      "('read', 'the', 'book', 'thoroughly', '.')\n",
      "('the', 'book', 'thoroughly', '.', 'If')\n",
      "('book', 'thoroughly', '.', 'If', 'you')\n",
      "('thoroughly', '.', 'If', 'you', 'have')\n",
      "('.', 'If', 'you', 'have', 'anyqueries')\n",
      "('If', 'you', 'have', 'anyqueries', ',')\n",
      "('you', 'have', 'anyqueries', ',', 'then')\n",
      "('have', 'anyqueries', ',', 'then', \"don't\")\n",
      "('anyqueries', ',', 'then', \"don't\", 'hesitate')\n",
      "(',', 'then', \"don't\", 'hesitate', 'to')\n",
      "('then', \"don't\", 'hesitate', 'to', 'ask')\n",
      "(\"don't\", 'hesitate', 'to', 'ask', '.')\n",
      "('hesitate', 'to', 'ask', '.', 'There')\n",
      "('to', 'ask', '.', 'There', 'is')\n",
      "('ask', '.', 'There', 'is', 'no')\n",
      "('.', 'There', 'is', 'no', 'shortcut')\n",
      "('There', 'is', 'no', 'shortcut', 'to')\n",
      "('is', 'no', 'shortcut', 'to', 'success.')\n"
     ]
    }
   ],
   "source": [
    "sent=\" Hello , please read the book thoroughly . If you have anyqueries , then don't hesitate to ask . There is no shortcut to success.\"\n",
    "n = 5\n",
    "n_grams=ngrams(sent.split(),n)\n",
    "for grams in n_grams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Model estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = nltk.corpus.brown.tagged_sents(categories='adventure')[:700]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import unique_list\n",
    "tag_set = unique_list(tag for sent in corpus for (word,tag) in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "print(len(tag_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = unique_list(word for sent in corpus for (word,tag) in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1908\n"
     ]
    }
   ],
   "source": [
    "print(len(symbols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)\n",
    "train_corpus = []\n",
    "test_corpus = []\n",
    "for i in range(len(corpus)):\n",
    "    if i % 10:\n",
    "        train_corpus += [corpus[i]]\n",
    "    else:\n",
    "        test_corpus += [corpus[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "630"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(est):\n",
    "    hmm = trainer.train_supervised(train_corpus, estimator=est)\n",
    "    print('%.2f%%' % (100 * hmm.evaluate(test_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding code, we have created a 90% training and 10% testing file and we\n",
    "have tested the estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Turing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Turing was introduced by Alan Turing along with his statistical assistant I.J.\n",
    "Good. It is an efficient smoothing method that increases the performance of statistical\n",
    "techniques performed for linguistic tasks, such as word sense disambiguation\n",
    "(WSD), named entity recognition (NER), spelling correction, machine translation,\n",
    "and so on. This method helps to predict the probability of unseen objects. In this\n",
    "method, binomial distribution is exhibited by our objects of interest. This method is\n",
    "used to compute the mass probability for zero or low count samples on the basis of\n",
    "higher count samples . Simple Good Turing performs approximation from frequency\n",
    "to frequency by linear regression into a linear line in log space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = lambda fd, bins: SimpleGoodTuringProbDist(fd, bins=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(fd, bins)>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bdhooda\\AppData\\Local\\conda\\conda\\envs\\dfhdfh\\lib\\site-packages\\nltk\\probability.py:1415: UserWarning: SimpleGoodTuring did not find a proper best fit line for smoothing probabilities of occurrences. The probability estimates are likely to be unreliable.\n",
      "  'SimpleGoodTuring did not find a proper best fit '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.74%\n"
     ]
    }
   ],
   "source": [
    "train_and_test(gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kneser Ney estimation\n",
    "Kneser Ney is used with trigrams. Let's see the following code in NLTK for the\n",
    "Kneser Ney estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [[((x[0],y[0],z[0]),(x[1],y[1],z[1])) for x, y, z in nltk.trigrams(sent)] for sent in corpus[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_set = unique_list(tag for sent in corpus for (word,tag) in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "906"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = unique_list(word for sent in corpus for (word,tag) in\n",
    "sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1341"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)\n",
    "train_corpus = []\n",
    "test_corpus = []\n",
    "for i in range(len(corpus)):\n",
    "    if i % 10:\n",
    "        train_corpus += [corpus[i]]\n",
    "    else:\n",
    "        test_corpus += [corpus[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86%\n"
     ]
    }
   ],
   "source": [
    "kn = lambda fd, bins: KneserNeyProbDist(fd)\n",
    "train_and_test(kn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Witten Bell estimation\n",
    "Witten Bell is the smoothing algorithm that was designed to deal with unknown\n",
    "words having zero probability. Let's consider the following code for Witten Bell\n",
    "estimation in NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.90%\n"
     ]
    }
   ],
   "source": [
    "train_and_test(WittenBellProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop a back-off mechanism for MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "dfhdfh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
