{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphology – Getting Our Feet Wet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphology may be defined as the study of the composition of words using morphemes. A morpheme is the smallest unit of language that has meaning. In this chapter, we will discuss stemming and lemmatizing, stemmer and lemmatizer for non-English languages, developing a morphological analyzer and morphological generator using machine learning tools, search engines, and many such concepts.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In brief, this chapter will include the following topics:\n",
    "•Introducing morphology\n",
    "•Understanding stemmer\n",
    "•Understanding lemmatization\n",
    "•Developing a stemmer for non-English languages\n",
    "•Morphological analyzer\n",
    "•Morphological generator\n",
    "•Search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphology may be defined as the study of the production of tokens with the\n",
    "help of morphemes. A morpheme is the basic unit of language carrying meaning. There are two types of morpheme: stems and affixes (suffixes, prefixes, infixes,\n",
    "and circumfixes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stems are also referred to as free morphemes, since they can even exist without\n",
    "adding affixes. Affixes are referred to as bound morphemes, since they cannot exist\n",
    "in a free form and they always exist along with free morphemes. Consider the word\n",
    "unbelievable. Here, believe is a stem or a free morpheme. It can exist on its own.\n",
    "The morphemes un and able are affixes or bound morphemes. They cannot exist\n",
    "in a free form, but they exist together with stem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three kinds of language,\n",
    "namely isolating languages, agglutinative languages, and inflecting languages.\n",
    "Morphology has a different meaning in all these languages. Isolating languages are\n",
    "those languages in which words are merely free morphemes and they do not carry\n",
    "any tense (past, present, and future) and number (singular or plural) information.\n",
    "Mandarin Chinese is an example of an isolating language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglutinative languages\n",
    "are those in which small words combine together to convey compound information.\n",
    "Turkish is an example of an agglutinative language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inflecting languages are\n",
    "those in which words are broken down into simpler units, but all these simpler\n",
    "units exhibit different meanings. Latin is an example of an inflecting language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphological processes are of the following types: inflection, derivation, semiaffixes\n",
    "and combining forms, and cliticization. Inflection means transforming the word into\n",
    "a form so that it represents person, number, tense, gender, case, aspect, and mood.\n",
    "Here, the syntactic category of a token remains the same. In derivation, the syntactic\n",
    "category of a word is also changed. Semiaffixes are bound morphemes that exhibit\n",
    "words, such as quality, for example, noteworthy, antisocial, anticlockwise, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming may be defined as the process of obtaining a stem from a word by\n",
    "eliminating the affixes from a word. For example, in the case of the word raining,\n",
    "stemmer would return the root word or stem word rain by removing the affix from\n",
    "raining. In order to increase the accuracy of information retrieval, search engines\n",
    "mostly use stemming to get the stems and store them as indexed words. Search\n",
    "engines call words with the same meaning synonyms, which may be a kind of query\n",
    "expansion known as conflation. Martin Porter has designed a well-known stemming\n",
    "algorithm known as the Porter stemming algorithm. This algorithm is basically\n",
    "designed to replace and eliminate some well-known suffices present in English\n",
    "words. To perform stemming in NLTK, we can simply do an instantiation of the\n",
    "PorterStemmer class and then perform stemming by calling the stem method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmerporter = PorterStemmer()\n",
    "stemmerporter.stem('working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happi'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmerporter.stem('happiness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of Stemmers:\n",
    "i)PorterStemmer\n",
    "ii)LancasterStemmer\n",
    "iii)RegExp Stemmer\n",
    "iv)SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LancasterStemmer \n",
    "Lancaster stemming algorithm was\n",
    "introduced at Lancaster University. Similar to the PorterStemmer class, the\n",
    "LancasterStemmer class is used in NLTK to implement Lancaster stemming.\n",
    "However, one of the major differences between the two algorithms is that Lancaster\n",
    "stemming involves the use of more words of different sentiments as compared to\n",
    "Porter Stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer_lan=LancasterStemmer()\n",
    "stemmer_lan.stem('working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer_lan.stem('happiness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RegexStemmer\n",
    "We can also build our own stemmer in NLTK using RegexpStemmer. It works by\n",
    "accepting a string and eliminating the string from the prefix or suffix of a word\n",
    "when a match is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "stemmer_regexp=RegexpStemmer('ing')\n",
    "stemmer_regexp.stem('working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happiness'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " stemmer_regexp.stem('happiness')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pair'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " stemmer_regexp.stem('pairing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnowballStemmer\n",
    "SnowballStemmer is used to perform stemming in 13 languages other than English.\n",
    "In order to perform stemming using SnowballStemmer, firstly, an instance is created\n",
    "in the language in which stemming needs to be performed. Then, using the stem()\n",
    "method, stemming is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'com'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanishstemmer=SnowballStemmer('spanish')\n",
    "spanishstemmer.stem('comiendo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mang'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frenchstemmer=SnowballStemmer('french')\n",
    "frenchstemmer.stem('manger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is the process in which we transform the word into a form with a\n",
    "different word category. The word formed after lemmatization is entirely different.\n",
    "The built-in morphy() function is used for lemmatization in WordNetLemmatizer.\n",
    "The inputted word is left unchanged if it is not found in WordNet. In the argument,\n",
    "pos refers to the part of speech category of the inputted word.\n",
    "\n",
    "Consider an example of lemmatization in NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'working'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output=WordNetLemmatizer()\n",
    "lemmatizer_output.lemmatize('working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer_output.lemmatize('working',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer_output.lemmatize('works')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a stemmer for non-English language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polyglot is a software that is used to provide models called morfessor models that\n",
    "are used to obtain morphemes from tokens. The Morpho project's goal is to create\n",
    "unsupervised data-driven processes. The main aim of the Morpho project is to focus\n",
    "on the creation of morphemes, which is the smallest unit of syntax. Morphemes\n",
    "play an important role in natural language processing. Morphemes are useful in\n",
    "automatic recognition and the creation of language. With the help of the vocabulary\n",
    "dictionaries of Polyglot, morfessor models on the 50,000 tokens of different\n",
    "languages were used.\n",
    "\n",
    "\n",
    "Let's see the code for obtaining the language table using polyglot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to Polyglot_Package_Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphological analysis may be defined as the process of obtaining grammatical\n",
    "information from tokens, given their suffix information. Morphological analysis can be\n",
    "performed in three ways: morpheme-based morphology (or anitem and arrangement\n",
    "approach), lexeme-based morphology (or an item and process approach), and wordbased\n",
    "morphology (or a word and paradigm approach). A morphological analyzer\n",
    "may be defined as a program that is responsible for the analysis of the morphology of a\n",
    "given input token. It analyzes a given token and generates morphological information,\n",
    "such as gender, number, class, and so on, as an output."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can determine the category of the word with the help of the following points:\n",
    "\n",
    "• Morphological hints: The suffix's information helps us detect the category\n",
    "of a word. For example, the -ness and –ment suffixes exist with nouns.\n",
    "\n",
    "• Syntactic hints: Contextual information is conducive to determine the\n",
    "category of a word. For example, if we have found the word that has the\n",
    "noun category, then syntactic hints will be useful for determining whether\n",
    "an adjective would appear before the noun or after the noun category.\n",
    "\n",
    "• Semantic hints: A semantic hint is also useful for determining the word's\n",
    "category. For example, if we already know that a word represents the name\n",
    "of a location, then it will fall under the noun category.\n",
    "\n",
    "• Open class: This is class of words that are not fixed, and their number keeps\n",
    "on increasing every day, whenever a new word is added to their list. Words\n",
    "in the open class are usually nouns. Prepositions are mostly in a closed class.\n",
    "For example, there can be an unlimited number of words in the of Persons\n",
    "list. So, it is an open class.\n",
    "\n",
    "• Morphology captured by the Part of Speech tagset: The Part of Speech\n",
    "tagset captures information that helps us perform morphology. For example,\n",
    "the word plays would appear with the third person and a singular noun.\n",
    "\n",
    "• Omorfi:Omorfi (Open morphology of Finnish) is a package that has\n",
    "been licensed by GNU GPL version 3. It is used for performing numerous\n",
    "tasks, such as language modeling, morphological analysis, rule-based\n",
    "machine translation, information retrieval, statistical machine translation,\n",
    "morphological segmentation, ontologies, and spell checking and correction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A morphological generator is a program that performs the task of morphological\n",
    "generation. Morphological generation may be considered an opposite task of\n",
    "morphological analysis. Here, given the description of a word in terms of number,\n",
    "category, stem, and so on, the original word is retrieved. For example, if root = go,\n",
    "part of speech = verb, tense= present, and if it occurs along with a third person and\n",
    "singular subject, then a morphological generator would generate its surface\n",
    "form, goes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of Python-based software that performs morphological analysis and\n",
    "generation. Some of them are as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "• ParaMorfo: It is used to perform morphological generation and analysis of\n",
    "Spanish and Guarani nouns, adjectives, and verbs.\n",
    "\n",
    "• HornMorpho: It is used for the morphological generation and analysis of\n",
    "Oromo and Amharic nouns and verbs, as well as Tigrinya verbs.\n",
    "\n",
    "• AntiMorfo: It is used for the morphological generation and analysis of\n",
    "Quechua adjectives, verbs, and nouns, as well as Spanish verbs.\n",
    "\n",
    "• MorfoMelayu: It is used for the morphological analysis of Malay words.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other examples of software that is used to perform morphological analysis and\n",
    "generation are as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "• Morph is a morphological generator and analyzer for English for the RASP\n",
    "system\n",
    "• Morphy is a morphological generator, analyzer, and POS tagger for German\n",
    "• Morphisto is a morphological generator and analyzer for German\n",
    "• Morfette performs supervised learning (inflectional morphology) for Spanish\n",
    "and French"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminatestopwords(self,list):\n",
    "    return[ word for word in list if word not in self.stopwords ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(self,string):\n",
    "    Str=self.clean(str)\n",
    "    Words=str.split(\" \")\n",
    "    return [self.stemmer.stem(word,0,len(word)-1) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainvectorkeywordindex(self, documentList):\n",
    "    vocabstring = \"\".join(documentList)\n",
    "    vocablist = self.parser.tokenise(vocabstring)\n",
    "    vocablist = self.parser.eliminatestopwords(vocablist)\n",
    "    uniqueVocablist = util.removeDuplicates(vocablist)\n",
    "    vectorIndex={}\n",
    "    offset=0\n",
    "    for word in uniqueVocablist:\n",
    "        vectorIndex[word]=offset\n",
    "        offset+=1\n",
    "    return vectorIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructVector(self, wordString):\n",
    "    Vector_val = [0] * len(self.vectorKeywordIndex)\n",
    "    tokList = self.parser.tokenize(tokString)\n",
    "    tokList = self.parser.eliminatestopwords(tokList)\n",
    "    for word in toklist:\n",
    "        vector[self.vectorKeywordIndex[word]] += 1;\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(vec1, vec2):\n",
    "    return float(dot(vec1,vec2) / (norm(vec1) * norm(vec2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searching(self,searchinglist):\n",
    "    askVector = self.buildQueryVector(searchinglist)\n",
    "    ratings = [util.cosine(askVector, textVector) for textVector in self.documentVectors]\n",
    "    ratings.sort(reverse=True)\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from nltk import wordpunct_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "except ImportError:\n",
    "    print( 'Error has occured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_languages_ratios(text):\n",
    "    languages_ratios = {}\n",
    "    tok = wordpunct_tokenize(text)\n",
    "    wor = [word.lower() for word in tok]\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(wor)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios[language] = len(common_elements)\n",
    "        return languages_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    ratios = _calculate_languages_ratios(text)\n",
    "    most_rated_language = max(ratios, key=ratios.get)\n",
    "    return most_rated_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "الثالث على التوالي عن التقدم الى داخل مدينة تكريت بسبب\n",
    "انتشار قناصي التنظيم الذي ي\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = detect_language(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic\n"
     ]
    }
   ],
   "source": [
    "print(language)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "dfhdfh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
